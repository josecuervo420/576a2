{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josecuervo420/576a2/blob/main/beckassignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2QCxjpgrFVWG"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import portalocker"
      ]
    },
    {
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import IMDB\n",
        "from collections import Counter\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter, special_tokens):\n",
        "    for token in special_tokens:\n",
        "        yield [token]\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "special_tokens = [\"<unk>\", \"<pad>\", \"<start>\", \"<end>\"]\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter, special_tokens), specials=special_tokens)\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ty2xCHMlIFcN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xcRLvvIUFVWJ"
      },
      "outputs": [],
      "source": [
        "def build_ngram_model(data, n=3):\n",
        "    model = defaultdict(Counter)\n",
        "    for sentence in data:\n",
        "        for i in range(len(sentence)-n+1):\n",
        "            context = tuple(sentence[i:i+n-1])\n",
        "            target = sentence[i+n-1]\n",
        "            model[context][target] += 1\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bnzmXrNFVWJ",
        "outputId": "7f16f2b4-0daa-46e4-ec9f-571def407f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lock acquired. Press Enter to release lock...\n",
            "/usr/bin/python3\n"
          ]
        }
      ],
      "source": [
        "import portalocker\n",
        "with open(\"test.lock\", \"w\") as lock_file:\n",
        "    portalocker.lock(lock_file, portalocker.LOCK_EX)\n",
        "    input(\"Lock acquired. Press Enter to release lock...\")\n",
        "\n",
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CR5ywhkzFVWJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_uPVsnXZFVWJ"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "\n",
        "def load_data(data_type='train'):\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "    counter = Counter()\n",
        "    for label, line in IMDB(split=data_type):\n",
        "        counter.update(tokenizer(line))\n",
        "    tokenized_text = [tok for tok, cnt in counter.items() for _ in range(cnt)]\n",
        "    return tokenized_text\n",
        "\n",
        "tokenized_text = load_data('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uerowftJFVWK"
      },
      "outputs": [],
      "source": [
        "def build_ngram_model(tokenized_text, n=3):\n",
        "    model = {}\n",
        "    for i in range(len(tokenized_text)-n):\n",
        "        gram = tuple(tokenized_text[i:i+n-1])\n",
        "        next_word = tokenized_text[i+n-1]\n",
        "        if gram not in model:\n",
        "            model[gram] = {}\n",
        "        if next_word not in model[gram]:\n",
        "            model[gram][next_word] = 0\n",
        "        model[gram][next_word] += 1\n",
        "    for gram in model.keys():\n",
        "        total = float(sum(model[gram].values()))\n",
        "        for word in model[gram]:\n",
        "            model[gram][word] /= total\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wSaMUwfzFVWK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_text(model, start_text, num_words=20, n=3):\n",
        "    result = start_text.split()\n",
        "    for _ in range(num_words):\n",
        "        state = tuple(result[-(n-1):])\n",
        "        next_words = model.get(state, None)\n",
        "        if not next_words:\n",
        "            break\n",
        "        next_word = random.choices(list(next_words.keys()), weights=next_words.values())[0]\n",
        "        result.append(next_word)\n",
        "    return ' '.join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_nPjMgpFVWK",
        "outputId": "d7cca806-7d56-4ab2-c6f2-e22f8c082aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My favorite movie\n",
            "My favorite movie\n",
            "My favorite movie\n",
            "My favorite movie\n",
            "My favorite movie\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = load_data('train')\n",
        "ngram_model = build_ngram_model(tokenized_text, n=3)\n",
        "\n",
        "for _ in range(5):\n",
        "    print(generate_text(ngram_model, \"My favorite movie\", num_words=20, n=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7omgFSEjFVWK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data_iter, vocab, tokenizer):\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "        for label, text in data_iter:\n",
        "            numericalized_text = [self.vocab.get('<start>', self.vocab.get('<unk>'))] + \\\n",
        "                                 [self.vocab.get(token, self.vocab.get('<unk>')) for token in self.tokenizer(text)] + \\\n",
        "                                 [self.vocab.get('<end>', self.vocab.get('<unk>'))]\n",
        "            self.data.append((numericalized_text, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        numericalized_text, label = self.data[idx]\n",
        "        input_sequence = torch.tensor(numericalized_text[:-1], dtype=torch.long)\n",
        "        target_sequence = torch.tensor(numericalized_text[1:], dtype=torch.long)\n",
        "        return input_sequence, target_sequence, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AH_389fOFVWL"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v0JZd8RuFVWL"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>', '<start>', '<end>'])\n",
        "\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KMVGJw5dFVWL"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmZo6A8bFVWL",
        "outputId": "cb16fcdd-8b54-4825-e38f-c04f2d50c041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-cced7a9194e1>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_sequences_padded = pad_sequence([torch.tensor(seq) for seq in input_sequences], batch_first=True, padding_value=0)\n",
            "<ipython-input-14-cced7a9194e1>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets_padded = pad_sequence([torch.tensor(tgt) for tgt in targets], batch_first=True, padding_value=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.9982561469078064\n",
            "Epoch 2, Loss: 2.9621703177690506\n",
            "Epoch 3, Loss: 2.918991297483444\n",
            "Epoch 4, Loss: 2.8303691744804382\n",
            "Epoch 5, Loss: 2.6860973089933395\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, input_sequences, targets):\n",
        "        self.input_sequences = input_sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_sequences[idx], self.targets[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_sequences, targets = zip(*batch)\n",
        "    input_sequences_padded = pad_sequence([torch.tensor(seq) for seq in input_sequences], batch_first=True, padding_value=0)\n",
        "    targets_padded = pad_sequence([torch.tensor(tgt) for tgt in targets], batch_first=True, padding_value=0)\n",
        "    return input_sequences_padded, targets_padded\n",
        "\n",
        "vocab_size = 10000\n",
        "num_classes = 20\n",
        "\n",
        "input_sequences = torch.randint(0, vocab_size, (1000, 10))\n",
        "targets = torch.randint(0, num_classes, (1000, 10))\n",
        "\n",
        "dataset = TextDataset(input_sequences, targets)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim=100, hidden_dim=256, num_classes=num_classes)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_sequences, targets in train_loader:\n",
        "        input_sequences, targets = input_sequences.long(), targets.long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(input_sequences)\n",
        "\n",
        "        loss = criterion(predictions.view(-1, num_classes), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    loss_values.append(average_loss)\n",
        "    print(f'Epoch {epoch+1}, Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = '/content/bul.txt'\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "else:\n",
        "    print(\"File not found. Please check the file path.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py6dYSNXNYKG",
        "outputId": "4a9e9661-afe3-42aa-a7b3-aa84036564c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "cxnSjUPzP3xM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_path = '/content/glove.6B.100d.txt'\n",
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(path):\n",
        "    embeddings = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "vocab = ['hello', 'world', '<unk>', '<pad>']\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "weights_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "    try:\n",
        "        weights_matrix[i] = glove_embeddings[word]\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "embedding_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKU0VfK2ib5s",
        "outputId": "d0bf85bb-9918-4b5c-bf40-e6ae7866ad74"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.6688e-01,  3.9632e-01,  6.1690e-01, -7.7451e-01, -1.0390e-01,\n",
              "          2.6697e-01,  2.7880e-01,  3.0992e-01,  5.4685e-03, -8.5256e-02,\n",
              "          7.3602e-01, -9.8432e-02,  5.4790e-01, -3.0305e-02,  3.3479e-01,\n",
              "          1.4094e-01, -7.0003e-03,  3.2569e-01,  2.2902e-01,  4.6557e-01,\n",
              "         -1.9531e-01,  3.7491e-01, -7.1390e-01, -5.1775e-01,  7.7039e-01,\n",
              "          1.0881e+00, -6.6011e-01, -1.6234e-01,  9.1190e-01,  2.1046e-01,\n",
              "          4.7494e-02,  1.0019e+00,  1.1133e+00,  7.0094e-01, -8.6960e-02,\n",
              "          4.7571e-01,  1.6360e-01, -4.4469e-01,  4.4690e-01, -9.3817e-01,\n",
              "          1.3101e-02,  8.5964e-02, -6.7456e-01,  4.9662e-01, -3.7827e-02,\n",
              "         -1.1038e-01, -2.8612e-01,  7.4606e-02, -3.1527e-01, -9.3774e-02,\n",
              "         -5.7069e-01,  6.6865e-01,  4.5307e-01, -3.4154e-01, -7.1660e-01,\n",
              "         -7.5273e-01,  7.5212e-02,  5.7903e-01, -1.1910e-01, -1.1379e-01,\n",
              "         -1.0026e-01,  7.1341e-01, -1.1574e+00, -7.4026e-01,  4.0452e-01,\n",
              "          1.8023e-01,  2.1449e-01,  3.7638e-01,  1.1239e-01, -5.3639e-01,\n",
              "         -2.5092e-02,  3.1886e-01, -2.5013e-01, -6.3283e-01, -1.1843e-02,\n",
              "          1.3770e+00,  8.6013e-01,  2.0476e-01, -3.6815e-01, -6.8874e-01,\n",
              "          5.3512e-01, -4.6556e-01,  2.7389e-01,  4.1180e-01, -8.5400e-01,\n",
              "         -4.6288e-02,  1.1304e-01, -2.7326e-01,  1.5636e-01, -2.0334e-01,\n",
              "          5.3586e-01,  5.9784e-01,  6.0469e-01,  1.3735e-01,  4.2232e-01,\n",
              "         -6.1279e-01, -3.8486e-01,  3.5842e-01, -4.8464e-01,  3.0728e-01],\n",
              "        [ 4.9177e-01,  1.1164e+00,  1.1424e+00,  1.4381e-01, -1.0696e-01,\n",
              "         -4.6727e-01, -4.4374e-01, -8.8024e-03, -5.0406e-01, -2.0549e-01,\n",
              "          5.0910e-01, -6.0904e-01,  2.0980e-01, -4.4836e-01, -7.0383e-01,\n",
              "          2.1516e-01,  6.6189e-01,  3.4620e-01, -8.9294e-01, -4.8032e-01,\n",
              "          4.3069e-01,  3.5697e-01,  8.4277e-01,  5.2344e-01,  8.2065e-01,\n",
              "          5.3183e-04,  2.4835e-01, -2.0887e-01,  8.1657e-01,  2.5048e-01,\n",
              "         -7.4761e-01, -1.1309e-02, -4.7481e-01,  6.4520e-02,  5.4517e-01,\n",
              "          2.0714e-01, -4.6237e-01,  1.0724e+00, -1.0526e+00, -1.5567e-01,\n",
              "         -7.9339e-01, -2.8366e-02,  1.0138e-01, -2.0909e-01,  4.5513e-01,\n",
              "          4.7330e-01,  6.8859e-01, -2.3840e-01, -5.5178e-02, -8.3022e-01,\n",
              "         -4.7127e-01,  2.2713e-01,  4.2651e-02,  1.1273e+00, -8.4776e-02,\n",
              "         -3.0378e+00, -1.8389e-01,  7.8244e-01,  1.6395e+00,  7.6146e-01,\n",
              "         -1.4258e-01,  6.5115e-01, -1.3549e-02, -5.1465e-01,  6.6951e-01,\n",
              "         -3.4464e-01, -1.4525e-01,  4.9258e-01,  8.0085e-01, -5.4971e-01,\n",
              "          3.9657e-01, -4.8571e-01, -4.3846e-01,  3.3180e-01,  1.0356e-01,\n",
              "         -2.8987e-02,  1.0896e-01, -4.5671e-01, -1.1150e+00, -8.2366e-02,\n",
              "          1.0186e+00,  3.0639e-02, -3.7162e-01,  1.0742e+00, -1.0642e+00,\n",
              "         -2.0298e-01, -9.8434e-01, -3.2040e-01,  1.5969e-01, -1.7910e-01,\n",
              "          2.1325e-01,  4.7155e-01,  6.8247e-01,  1.3784e-01, -1.0704e-01,\n",
              "         -1.8294e-01, -4.0082e-01, -5.0885e-01,  6.2556e-01,  4.3917e-01],\n",
              "        [ 1.0946e+00, -7.1468e-01, -1.9700e-01, -1.9685e-01,  2.3780e-01,\n",
              "          5.6145e-01, -1.1290e-01,  4.5559e-01, -4.5220e-01, -6.0576e-03,\n",
              "          1.2599e-01, -1.1087e+00, -4.4578e-01,  6.1821e-01, -1.9824e-02,\n",
              "          3.1710e-01,  8.3318e-01,  4.0351e-01, -1.7987e-01, -3.3960e-01,\n",
              "         -1.3188e-01, -8.4103e-02, -5.0788e-01,  5.5398e-01,  1.0562e+00,\n",
              "         -2.3055e-01, -1.8169e-01,  3.9784e-02,  2.0033e-01,  5.5144e-01,\n",
              "          2.0780e-02, -2.1634e-01, -7.7230e-01,  5.2648e-01, -4.0605e-01,\n",
              "          1.8462e-02, -3.6946e-01, -6.6686e-01, -7.6902e-01,  1.9318e-01,\n",
              "         -7.6544e-02, -5.5906e-03,  6.0673e-01,  2.4409e-01,  5.8862e-01,\n",
              "          3.8136e-01, -2.3792e-01, -8.5358e-01, -1.0394e+00, -6.4594e-01,\n",
              "          8.4739e-01,  7.6315e-01, -1.1961e-01,  4.9452e-01,  4.3140e-01,\n",
              "         -6.0695e-01,  4.6634e-01,  5.7880e-02, -5.8380e-02, -1.7210e-01,\n",
              "         -7.5361e-01,  5.8328e-01, -3.4473e-01, -3.6269e-01,  5.3372e-01,\n",
              "          1.0258e+00,  1.4560e-01, -2.2055e-01, -5.6690e-01,  7.3082e-01,\n",
              "          7.3301e-02,  1.1747e-01,  6.4284e-01, -1.1158e-01, -1.8847e-01,\n",
              "         -8.9096e-02,  5.6222e-01,  5.8003e-01, -6.5999e-01,  4.2113e-01,\n",
              "         -5.4833e-01, -4.8847e-01, -5.5042e-01, -6.0615e-01, -2.9671e-01,\n",
              "         -7.0145e-01,  9.0358e-01,  8.7267e-01, -9.4562e-02,  7.1922e-03,\n",
              "          4.7442e-01, -5.8005e-01, -6.7139e-01, -6.6972e-01,  6.5970e-01,\n",
              "         -2.2847e-01, -4.5236e-01,  8.4712e-01,  3.0369e-01,  8.0947e-02],\n",
              "        [ 5.9385e-03, -6.5864e-02, -6.9035e-01,  3.2296e-01,  1.1210e+00,\n",
              "         -9.2278e-01,  9.2450e-01, -1.0084e-01, -1.2560e+00, -3.4399e-01,\n",
              "         -2.2269e-01,  1.0349e+00,  4.6456e-01, -4.0242e-01, -7.9873e-01,\n",
              "         -2.9091e-01,  2.0649e-01,  3.5177e-01,  9.7952e-01,  2.0278e-01,\n",
              "          6.2883e-01,  1.6105e-01, -1.0576e+00,  4.0221e-01,  2.2172e-01,\n",
              "          4.4464e-01, -1.0484e+00, -4.4628e-01, -6.5146e-01, -1.6265e+00,\n",
              "         -7.8808e-01,  6.0947e-01,  1.1951e+00, -4.0201e-01, -4.2811e-01,\n",
              "          8.7164e-02,  8.6784e-02, -6.0794e-01, -6.8900e-01,  3.2401e-01,\n",
              "         -4.6464e-01,  5.2975e-01, -3.8514e-01,  6.2350e-01, -5.5730e-02,\n",
              "         -1.1377e-01,  9.9082e-01,  9.6858e-01,  1.5389e-01,  7.7722e-01,\n",
              "         -1.8425e-01,  2.1231e-02, -6.5993e-01, -5.1965e-01,  6.0961e-01,\n",
              "          2.1163e-02,  2.9648e-01, -7.7709e-01,  3.1294e-01, -1.4898e+00,\n",
              "          2.5131e-01, -5.0930e-02,  2.9840e-01,  5.4199e-01, -2.5453e-01,\n",
              "          3.0722e-01, -5.3129e-01,  1.3785e+00, -1.0496e+00, -2.4401e-01,\n",
              "          2.6373e-01, -1.3764e+00,  3.6048e-01,  1.6445e+00, -5.0203e-01,\n",
              "         -2.4753e-01, -3.1443e-01,  4.8794e-01,  6.0803e-01,  6.3791e-01,\n",
              "          1.6079e-01, -6.4269e-01,  2.2750e-02,  7.6340e-01, -7.5374e-01,\n",
              "         -9.9699e-01,  4.3661e-01,  4.0907e-01,  5.1828e-02, -8.1593e-01,\n",
              "         -2.6483e-01, -7.1137e-01, -3.4360e-02,  1.2556e-01, -1.8808e-01,\n",
              "         -6.3657e-01, -4.2333e-01,  5.7417e-02,  7.6566e-01, -3.9397e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mypytorchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}