{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0oIu49AW35VggT98ChKLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josecuervo420/576a2/blob/main/vanmini2p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader popular\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "id": "ymo3tgqY52FY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ba7377-c003-4991-d024-70909dbb4845"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uVD2_uSt5u6d"
      },
      "outputs": [],
      "source": [
        "## first attempt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1\n",
        "## van\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Lemmatize the input text and return a list of lemmatized words.\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    return [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "def calculate_consecutive_overlap_score(context, definition):\n",
        "    \"\"\"Calculate score with extra points for consecutive word matches.\"\"\"\n",
        "    context_tokens = lemmatize_text(context)\n",
        "    definition_tokens = lemmatize_text(definition)\n",
        "    max_score = 0\n",
        "    context_len = len(context_tokens)\n",
        "    definition_len = len(definition_tokens)\n",
        "\n",
        "    for i in range(context_len):\n",
        "        for j in range(definition_len):\n",
        "            score = 0\n",
        "            while i + score < context_len and j + score < definition_len and \\\n",
        "                  context_tokens[i + score] == definition_tokens[j + score]:\n",
        "                score += 1\n",
        "                max_score += score\n",
        "\n",
        "    return max_score\n",
        "\n",
        "def normalize_scores(senses_scores):\n",
        "    \"\"\"Normalize scores into a probability distribution.\"\"\"\n",
        "    total_score = sum(score for _, score in senses_scores)\n",
        "    if total_score == 0:\n",
        "        return [(sense, 0) for sense, _ in senses_scores]\n",
        "    return [(sense, score / total_score) for sense, score in senses_scores]\n",
        "\n",
        "def get_top_senses(context_sentence, target_word, score_function, n=3):\n",
        "    \"\"\"Determine the top N senses based on context, rewarding consecutive overlaps.\"\"\"\n",
        "    senses_scores = []\n",
        "    for sense in wn.synsets(target_word):\n",
        "        definition = sense.definition()\n",
        "        score = score_function(context_sentence, definition)\n",
        "        senses_scores.append((sense, score))\n",
        "\n",
        "    senses_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return normalize_scores(senses_scores)[:n]\n",
        "\n",
        "def wsd_with_embeddings(context_sentence, target_word, n=3):\n",
        "    \"\"\"Enhance WSD using word embeddings for semantic similarity.\"\"\"\n",
        "    senses_similarity = []\n",
        "    context_vec = nlp(context_sentence)\n",
        "    for sense in wn.synsets(target_word):\n",
        "        sense_definition = sense.definition()\n",
        "        sense_vec = nlp(sense_definition)\n",
        "        similarity = context_vec.similarity(sense_vec)\n",
        "        senses_similarity.append((sense, similarity))\n",
        "\n",
        "    senses_similarity.sort(key=lambda x: x[1], reverse=True)\n",
        "    return senses_similarity[:n]\n",
        "\n",
        "context_sentence = \"I looked at the pine and admired its tall stature.\"\n",
        "target_word = \"pine\"\n",
        "\n",
        "top_senses = get_top_senses(context_sentence, target_word, calculate_consecutive_overlap_score, n=3)\n",
        "print(\"Top senses based on enhanced overlap score:\")\n",
        "for sense, confidence in top_senses:\n",
        "    print(f\"Sense: {sense.definition()}, Confidence: {confidence:.4f}\")\n",
        "\n",
        "top_senses_with_embeddings = wsd_with_embeddings(context_sentence, target_word, n=3)\n",
        "print(\"\\nTop senses with embeddings:\")\n",
        "for sense, similarity in top_senses_with_embeddings:\n",
        "    print(f\"Sense: {sense.definition()}, Similarity: {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnBabjtF53Tq",
        "outputId": "757a8341-b58b-440b-b428-bbdc22b35756"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top senses based on enhanced overlap score:\n",
            "Sense: straight-grained durable and often resinous white to yellowish timber of any of numerous trees of the genus Pinus, Confidence: 1.0000\n",
            "Sense: a coniferous tree, Confidence: 0.0000\n",
            "Sense: have a desire for something or someone who is not present, Confidence: 0.0000\n",
            "\n",
            "Top senses with embeddings:\n",
            "Sense: straight-grained durable and often resinous white to yellowish timber of any of numerous trees of the genus Pinus, Similarity: 0.7330\n",
            "Sense: have a desire for something or someone who is not present, Similarity: 0.5875\n",
            "Sense: a coniferous tree, Similarity: 0.5318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## first attempt output :\n",
        "\n",
        "#\n",
        "\n",
        "## Mounted at /content/drive\n",
        "## [nltk_data] Downloading package wordnet to /root/nltk_data...\n",
        "## [nltk_data]   Package wordnet is already up-to-date!\n",
        "## [nltk_data] Downloading package punkt to /root/nltk_data...\n",
        "## [nltk_data]   Package punkt is already up-to-date!\n",
        "## Top senses based on enhanced overlap score:\n",
        "## Sense: straight-grained durable and often resinous white to yellowish timber of any of numerous trees of the genus Pinus, Confidence: 1.0000\n",
        "## Sense: a coniferous tree, Confidence: 0.0000\n",
        "## Sense: have a desire for something or someone who is not present, Confidence: 0.0000\n",
        "\n",
        "## Top senses with embeddings:\n",
        "## Sense: straight-grained durable and often resinous white to yellowish timber of any of numerous trees of the genus Pinus, Similarity: 0.7330\n",
        "## Sense: have a desire for something or someone who is not present, Similarity: 0.5875\n",
        "## Sense: a coniferous tree, Similarity: 0.5318"
      ],
      "metadata": {
        "id": "mKlQecys6acw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## second attempt"
      ],
      "metadata": {
        "id": "lZVRzMRF58f1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## van 2\n",
        "#\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def load_dictionary(dictionary_path):\n",
        "    try:\n",
        "        tree = ET.parse(dictionary_path)\n",
        "        root = tree.getroot()\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"XML Parse Error: {e}\")\n",
        "        return {}\n",
        "\n",
        "    dictionary = {}\n",
        "    try:\n",
        "        for entry in root.findall('entry'):\n",
        "            word = entry.find('word').text\n",
        "            senses = []\n",
        "            for sense in entry.findall('sense'):\n",
        "                definition = sense.find('definition').text\n",
        "                examples = [ex.text for ex in sense.findall('example')]\n",
        "                senses.append({'definition': definition, 'examples': examples})\n",
        "            dictionary[word] = senses\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dictionary: {e}\")\n",
        "    return dictionary\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    return [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "def calculate_consecutive_overlap_score(context, definition, examples=[]):\n",
        "    context_tokens = lemmatize_text(context)\n",
        "    all_text = ' '.join([definition] + examples)\n",
        "    definition_tokens = lemmatize_text(all_text)\n",
        "    max_score = 0\n",
        "    context_len = len(context_tokens)\n",
        "    definition_len = len(definition_tokens)\n",
        "\n",
        "    for i in range(context_len):\n",
        "        for j in range(definition_len):\n",
        "            score = 0\n",
        "            while i + score < context_len and j + score < definition_len and \\\n",
        "                  context_tokens[i + score] == definition_tokens[j + score]:\n",
        "                score += 1\n",
        "                max_score += score**2\n",
        "    return max_score\n",
        "\n",
        "def normalize_scores(senses_scores):\n",
        "    total_score = sum(score for _, score in senses_scores)\n",
        "    normalized_scores = [(sense, score / total_score if total_score > 0 else 0) for sense, score in senses_scores]\n",
        "    return normalized_scores\n",
        "\n",
        "def wsd_with_embeddings(context_sentence, target_word):\n",
        "    senses_similarity = []\n",
        "    context_vec = nlp(context_sentence)\n",
        "    for sense in wn.synsets(target_word):\n",
        "        sense_definition = sense.definition()\n",
        "        sense_vec = nlp(sense_definition)\n",
        "        similarity = context_vec.similarity(sense_vec)\n",
        "        senses_similarity.append((sense, similarity))\n",
        "    senses_similarity.sort(key=lambda x: x[1], reverse=True)\n",
        "    return senses_similarity[:3]\n",
        "\n",
        "def load_dataset(dataset_path):\n",
        "    dataset = []\n",
        "    with open(dataset_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 3:\n",
        "                dataset.append(parts)\n",
        "            else:\n",
        "                print(f\"Skipping invalid data entry: {line}\")\n",
        "    return dataset\n",
        "\n",
        "def evaluate_wsd_system(test_data, wsd_function):\n",
        "    correct_predictions = 0\n",
        "    for data in test_data:\n",
        "        context_sentence, target_word, true_sense = data\n",
        "        predicted_senses = wsd_function(context_sentence, target_word)\n",
        "        if predicted_senses and predicted_senses[0][0].lemmas()[0].key() == true_sense:\n",
        "            correct_predictions += 1\n",
        "    accuracy = correct_predictions / len(test_data) if test_data else 0\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "dictionary_path = '/content/dictionary.xml'\n",
        "train_data_path = '/content/train.data'\n",
        "validate_data_path = '/content/validate.data'\n",
        "test_data_path = '/content/test.data'\n",
        "\n",
        "dictionary = load_dictionary(dictionary_path)\n",
        "train_data = load_dataset(train_data_path)\n",
        "validate_data = load_dataset(validate_data_path)\n",
        "test_data = load_dataset(test_data_path)\n",
        "\n",
        "sample_test_data = test_data[:10]\n",
        "evaluate_wsd_system(sample_test_data, wsd_with_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "JfP4aLlR5-TW",
        "outputId": "d5a05449-39ec-431b-cb37-25cafc22576b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dictionary.xml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-153f5f33b4c1>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mtest_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/test.data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mvalidate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-153f5f33b4c1>\u001b[0m in \u001b[0;36mload_dictionary\u001b[0;34m(dictionary_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \"\"\"\n\u001b[1;32m   1221\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dictionary.xml'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## second attempt output :\n",
        "\n",
        "#\n",
        "\n",
        "## ......\n",
        "## Skipping invalid data entry: defense.n | 1 | Last year , the Supreme Court defined when companies , such as military contractors , may defend themselves against lawsuits for deaths or injuries by asserting that they were simply following specifications of a federal government contract . In that decision , the high court said a company must prove that the government approved precise specifications for the contract , that those specifications were met and that the government was warned of any dangers in use of the equipment . But last February , a federal appeals court in New Orleans upheld a damage award against General Dynamics , rejecting the company 's use of the government contractor %% defense %% . The appeals court said the defense is valid only if federal officials did more than rubber stamp a company 's design or plans and engaged in a `` substantive review and evaluation '' on a par with a policy decision . General Dynamics appealed to the high court , backed by numerous business trade groups , arguing that the appeals court definition restricts the defense too severely .\n",
        "## Skipping invalid data entry: defense.n | 1 | In that decision , the high court said a company must prove that the government approved precise specifications for the contract , that those specifications were met and that the government was warned of any dangers in use of the equipment . But last February , a federal appeals court in New Orleans upheld a damage award against General Dynamics , rejecting the company 's use of the government contractor defense . The appeals court said the %% defense %% is valid only if federal officials did more than rubber stamp a company 's design or plans and engaged in a `` substantive review and evaluation '' on a par with a policy decision . General Dynamics appealed to the high court , backed by numerous business trade groups , arguing that the appeals court definition restricts the defense too severely . General Dynamics was sued by the families of five Navy divers who were killed in 1982 after they re-entered a submarine through a diving chamber .\n",
        "## Skipping invalid data entry: defense.n | 1 | But last February , a federal appeals court in New Orleans upheld a damage award against General Dynamics , rejecting the company 's use of the government contractor defense . The appeals court said the defense is valid only if federal officials did more than rubber stamp a company 's design or plans and engaged in a `` substantive review and evaluation '' on a par with a policy decision . General Dynamics appealed to the high court , backed by numerous business trade groups , arguing that the appeals court definition restricts the %% defense %% too severely . General Dynamics was sued by the families of five Navy divers who were killed in 1982 after they re-entered a submarine through a diving chamber . The accident was caused by faulty operation of a valve .\n",
        "\n",
        "## IOPub data rate exceeded.\n",
        "## The notebook server will temporarily stop sending output\n",
        "## to the client in order to avoid crashing it.\n",
        "## To change this limit, set the config variable\n",
        "## `--NotebookApp.iopub_data_rate_limit`.\n",
        "\n",
        "## Current values:\n",
        "## NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
        "## NotebookApp.rate_limit_window=3.0 (secs)\n"
      ],
      "metadata": {
        "id": "9WBQsC5P6wll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## van 3\n",
        "\n",
        "## 3\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Lemmatize the input text and return a list of lemmatized words.\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    return [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "def calculate_consecutive_overlap_score(context, definition, examples=[]):\n",
        "    \"\"\"Calculate score with extra points for consecutive word matches, enhanced by considering examples.\"\"\"\n",
        "    context_tokens = lemmatize_text(context)\n",
        "    all_text = ' '.join([definition] + examples)\n",
        "    definition_tokens = lemmatize_text(all_text)\n",
        "    max_score = 0\n",
        "    for i, context_token in enumerate(context_tokens):\n",
        "        for j, definition_token in enumerate(definition_tokens):\n",
        "            score = 0\n",
        "            while i + score < len(context_tokens) and j + score < len(definition_tokens) and \\\n",
        "                  context_tokens[i + score] == definition_tokens[j + score]:\n",
        "                score += 1\n",
        "                max_score += score**2\n",
        "    return max_score\n",
        "\n",
        "def wsd_with_embeddings(context_sentence, target_word, n=3):\n",
        "    \"\"\"Word Sense Disambiguation using word embeddings for semantic similarity.\"\"\"\n",
        "    senses_similarity = []\n",
        "    context_vec = nlp(context_sentence)\n",
        "    for sense in wn.synsets(target_word):\n",
        "        sense_definition = sense.definition()\n",
        "        sense_vec = nlp(sense_definition)\n",
        "        similarity = context_vec.similarity(sense_vec)\n",
        "        senses_similarity.append((sense, similarity))\n",
        "    senses_similarity.sort(key=lambda x: x[1], reverse=True)\n",
        "    return senses_similarity[:n]\n",
        "\n",
        "def load_dictionary(dictionary_path):\n",
        "    \"\"\"Load a custom XML-based dictionary.\"\"\"\n",
        "    try:\n",
        "        tree = ET.parse(dictionary_path)\n",
        "        root = tree.getroot()\n",
        "    except ET.ParseError as e:\n",
        "        print(f\"XML Parse Error: {e}\")\n",
        "        return {}\n",
        "    dictionary = {}\n",
        "    for entry in root.findall('entry'):\n",
        "        word = entry.find('word').text\n",
        "        senses = [{'definition': sense.find('definition').text, 'examples': [ex.text for ex in sense.findall('example')]} for sense in entry.findall('sense')]\n",
        "        dictionary[word] = senses\n",
        "    return dictionary\n",
        "\n",
        "context_sentence = \"I looked at the pine and admired its tall stature.\"\n",
        "target_word = \"pine\"\n",
        "top_senses_with_embeddings = wsd_with_embeddings(context_sentence, target_word, n=3)\n",
        "print(\"\\nTop senses with embeddings:\")\n",
        "for sense, similarity in top_senses_with_embeddings:\n",
        "    print(f\"Sense: {sense.definition()}, Similarity: {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4gZFUxDQ0f",
        "outputId": "9bc5792b-4b8c-4db9-cbf0-fc65b5001386"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top senses with embeddings:\n",
            "Sense: straight-grained durable and often resinous white to yellowish timber of any of numerous trees of the genus Pinus, Similarity: 0.7330\n",
            "Sense: have a desire for something or someone who is not present, Similarity: 0.5875\n",
            "Sense: a coniferous tree, Similarity: 0.5318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## third code output:\n",
        "#\n",
        "## Top senses with embeddings:\n",
        "## Sense: straight-grained durable and often resinous white to yellowish timber of any of numerous trees of the genus Pinus, Similarity: 0.7330\n",
        "## Sense: have a desire for something or someone who is not present, Similarity: 0.5875\n",
        "## Sense: a coniferous tree, Similarity: 0.5318"
      ],
      "metadata": {
        "id": "6MSAMXp7DZIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response 3 beats the other two for various reasons:\n",
        "\n",
        "Comprehensive Functionality: It combines lemmatization, WSD using overlap scores and embeddings, and XML dictionary loading from the first two responses. This complete approach makes NLP more robust and flexible.\n",
        "Better Scoring Mechanism: Including instances in WSD scoring improves sense disambiguation by taking context into account.\n",
        "Efficiency and Usability Improvements: Quiet flags during NLTK downloads and simplified XML parsing for dictionary loading show efficiency and user experience.\n",
        "Clarity and Organization: The code is well-organized and annotated, making it approachable to Python and NLP beginners.\n",
        "Response 3 is the finest solution since it integrates user-centric changes with greater capabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "3r0SCN-TD5HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition from Code 1 to Code 3 in the context of natural language processing (NLP) activities demonstrates a sequence of improvements targeted at boosting functionality, accuracy, and user experience.\n",
        "\n",
        "- **Code 1** lays the groundwork for basic word sense disambiguation (WSD) functions, using NLTK for lemmatization and Spacy for semantic similarity computations. It focuses on lemmatizing text and calculating consecutive overlap scores to aid in sense determination.\n",
        "\n",
        "- **Code 2** expands on this foundation by introducing important features such as XML dictionary loading for a more customizable approach to WSD, an improved scoring mechanism that squares the score for consecutive matches to emphasize longer overlaps, and dataset evaluation functionality for assessing system accuracy.\n",
        "\n",
        "- **Code 3** improves and incorporates the greatest elements from prior editions, resulting in a more streamlined and efficient approach. It preserves WSD's fundamental features by employing both overlap scores and embeddings, offers silent flags for less obtrusive NLTK downloads, and stresses code organization and comments that are clear and efficient.\n",
        "\n",
        "The transition from Code 1 to Code 3 is an incremental effort to refine and enhance the system's capabilities, improving adaptability and user-friendliness for tasks like word sense disambiguation and natural language processing. This step includes adding more features like loading XML dictionaries and evaluating datasets, improving the score system to make it more accurate, and making the system easier to use by doing things like reducing output clutter during setup. Each phase builds on the preceding one, resulting in a more powerful, adaptable, and efficient tool for dealing with complex NLP problems.\n"
      ],
      "metadata": {
        "id": "4mZd56akFj58"
      }
    }
  ]
}